{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efd892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if u are facing computation problems then use google colab,,which provide online resources(gpu,tpu,cpu,ram..etc)\n",
    "#to train model fastly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff678d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref=zipfile.ZipFile(r\"D:\\DEEP_LEARNING\\C_N_N\\4_cats_vs_dog_classification\\archive.zip\",'r')\n",
    "zip_ref.extractall()#extracting all contents of archive.zip in cwd\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86b764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84cc0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have to give i/p to cnn ie;train data,,,\n",
    "#1 way is that(manually)(customize code),,using os model go inside train data folder and go inside cat and dog folder,then\n",
    "#fetch the filename and give to cnn,,,and bcz images are more(nearly 20k), passing all these fetched cat and dog image from\n",
    "#training data is become tidious task bcz of  ram shortage ,gpu shortage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965a95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras provides the Generator concept\n",
    "#which generally divide the train data into multiple batches,,and at a time 1 batch is loaded into ram after processing\n",
    "#on that batch is done then it is removed from ram and next batch is loaded,,,and so on untill all batches processing completes..\n",
    "#after completion finally get optimum value for filters(w) and bias for each filter\n",
    "#now u can test data\n",
    "\n",
    "#therefore Genrator are used to process large amount of data,,if u have shortage of ram,gpu,tpu..etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73601104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Generator\n",
    "train_ds=keras.utils.image_dataset_from_directory(\n",
    "            directory = r\"D:\\DEEP_LEARNING\\C_N_N\\4_cats_vs_dog_classification\\train\",\n",
    "            labels='inferred',\n",
    "            label_mode='int',\n",
    "            batch_size=32,\n",
    "            image_size=(256,256)\n",
    ")\n",
    "#infer-->tarkane(not date) anuman kadne(marathi)/suchvne(denote)\n",
    "#ds means dataset->for our notation\n",
    "#label_mode='int'  cat folder  is 1st in train folder so -->0,,dog is 2nd folder so -->1\n",
    "#batch_size=32,-->at a time 32 images are loaded into ram and given as i/p to cnn\n",
    "#image_size=(256,256)--->bcz,all images are in diff resolution,,,thus training is not occurs good,,so make them in one size\n",
    "#ie;256,256 and keras automatically understand that images are RGB(colorfull) so,internally use 3 channels(256x256x3) for each image\n",
    "\n",
    "validation_ds=keras.utils.image_dataset_from_directory(\n",
    "            directory = r\"D:\\DEEP_LEARNING\\C_N_N\\4_cats_vs_dog_classification\\test\",\n",
    "            labels='inferred',\n",
    "            label_mode='int',\n",
    "            batch_size=32,\n",
    "            image_size=(256,256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b013c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found 20000 files belonging to 2 classes.-->train_ds,,,1class is cat(0) and another class is dog(1)\n",
    "#20k images are there intotal,,cat images are around 10k and dog=10k\n",
    "# Found 5000 files belonging to 2 classes.-->test dataset(validation_ds),,cat(0),,dog(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686ab910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20k images(pixel) value stored in train_ds in the form of numpy array,,ie;colorfull image so 3d array used\n",
    "#[]-->1d,,[[],[]]-->2d,,,[ [ [] , [] , [] ] , [ [] , [] , [] ] ]-->3d (2x3x3) \n",
    "#ie;256 rows and 256 cols and like this 3 matrix  are there(bcz RGB channel(3 channels))\n",
    "#[[c1,c2,...c256],[c1,c2,...c256],........,[c1,c2,...c256]]  -->(256x256),,,grayscale image  for 1 image\n",
    "#  r1             r2                       r256\n",
    "\n",
    "#for RGB   ,,\n",
    "#[ [ [c1,c2,...c256],[c1,c2,...c256],........,[c1,c2,...c256] ] ,[ [c1,c2,...c256],[c1,c2,...c256],........,[c1,c2,...c256] ] , [ [c1,c2,...c256],[c1,c2,...c256],........,[c1,c2,...c256] ]]\n",
    "#  R  r1             r2                        r256              G  r1              r2                       r256               B  r1              r2                       r256\n",
    "#(3x256x256)  for 1 image\n",
    "#and such a 20k images are there in tain_ds thus it is represented within a LIST,,,,,or we can say 4d array\n",
    "#(20000x3x256x256)-->20k images each with 3 cahnnels(matrix) and each matrix(channel) with 256 rows x 256 col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bf0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and pixel values are 0 to 255 ,,,therefore squish them in range 0 to 1 and it should be float,,,,\n",
    "#Normalization require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3735c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "def process(image,label):\n",
    "    image=tf.cast(image/255.,tf.float32)\n",
    "    return image,label\n",
    "train_ds=train_ds.map(process)#thus each value is 0 to 1 floating pt value\n",
    "validation_ds=validation_ds.map(process)\n",
    "#map will fetch one image and label from object and pass it to process func,,,repeats untill all images are processed\n",
    "#now all pixel values are in b/n 0 to 1 floating value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab266fb8",
   "metadata": {},
   "source": [
    "# create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a4ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()#1 layer after another,,so need to mention i/p shape and type for each layer,,model will automatically INFER it,,,bcz it is sequential model\n",
    "\n",
    "#C_L1\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "#C_L2\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "#C_L3\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))#bcz binary classification,,so 1 node at o.p layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03676224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 115200)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               14745728  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14847297 (56.64 MB)\n",
      "Trainable params: 14847297 (56.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757a6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total params: 14847297 (56.64 MB)-->1.48 cr parameters nad that are trainable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189f1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61425469",
   "metadata": {},
   "outputs": [],
   "source": [
    "myhistory=model.fit(train_ds,epochs=10,validation_data=validation_ds)\n",
    "#if u run this in Jupyter notebook ,,then it takes lot-lot of time\n",
    "#ie;32 batches  means 20k/32==>625 batches are there ,,and also 10 epochs\n",
    "#in every epochs u have to train 625 batches,,,and for ur kind info -->to train 1 epoch completely require more than 2hrs\n",
    "#so use Google colab,,and set runtime on GPU,,therefore with 1min ,,all 10 epochs completes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
